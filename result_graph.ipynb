{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a804ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Paths & file loading\n",
    "\n",
    "METRICS_DIR = Path(\n",
    "    r\"E:\\2025 fall\\Fundamentals of Digital Image Processing\\YOLO11\\runs\\test_metrics\"\n",
    ")\n",
    "\n",
    "txt_files = sorted(METRICS_DIR.glob(\"*.txt\"))\n",
    "print(\"Found txt files:\")\n",
    "for f in txt_files:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "\n",
    "\n",
    "# Parser for one txt file\n",
    "\n",
    "def parse_metrics_from_txt(path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parse one metrics txt file like:\n",
    "\n",
    "    AP@[.5:.95](B): 0.767013\n",
    "    AP@0.5(B)     : 0.902827\n",
    "    AP@0.75(B)    : 0.830418\n",
    "    mean P(B)     : 0.865376\n",
    "    mean R(B)     : 0.910000\n",
    "    mean F1(B)    : 0.887127\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"AP\":    None,\n",
    "        \"AP50\":  None,\n",
    "        \"AP75\":  None,\n",
    "        \"P\":     None,\n",
    "        \"R\":     None,\n",
    "        \"F1\":    None,\n",
    "    }\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            line_no_space = line.replace(\" \", \"\")\n",
    "\n",
    "            if \"AP@[.5:.95]\" in line_no_space or \"mAP@[.5:.95]\" in line_no_space:\n",
    "                metrics[\"AP\"] = float(line.split()[-1])\n",
    "            elif \"AP@0.5\" in line_no_space:\n",
    "                metrics[\"AP50\"] = float(line.split()[-1])\n",
    "            elif \"AP@0.75\" in line_no_space:\n",
    "                metrics[\"AP75\"] = float(line.split()[-1])\n",
    "            elif \"meanP(B)\" in line_no_space:\n",
    "                metrics[\"P\"] = float(line.split()[-1])\n",
    "            elif \"meanR(B)\" in line_no_space:\n",
    "                metrics[\"R\"] = float(line.split()[-1])\n",
    "            elif \"meanF1(B)\" in line_no_space:\n",
    "                metrics[\"F1\"] = float(line.split()[-1])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "# keep 5 baseline\n",
    "\n",
    "IP_KEYWORDS = (\n",
    "    \"finetune\", \"finetuned\", \"fine_tune\", \"fine tune\",\n",
    "    \"traditional\", \"fbcnn\", \"nafnet\", \"zerodce\"\n",
    ")\n",
    "\n",
    "def base_condition_from_filename(path: Path):\n",
    "\n",
    "    name = path.stem.lower()\n",
    "\n",
    "    if any(k in name for k in IP_KEYWORDS):\n",
    "        return None\n",
    "\n",
    "    if \"test_d\" in name:\n",
    "        return \"D\"\n",
    "    if \"raw\" in name:\n",
    "        return \"D'\"\n",
    "    if \"low\" in name and \"resolution\" in name:\n",
    "        return \"Low Resolution\"\n",
    "    if \"motion\" in name and \"blur\" in name:\n",
    "        return \"Motion Blur\"\n",
    "    if \"low\" in name and \"light\" in name:\n",
    "        return \"Low Light\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "rows_global = []\n",
    "for path in txt_files:\n",
    "    cond = base_condition_from_filename(path)\n",
    "    if cond is None:\n",
    "        continue\n",
    "    m = parse_metrics_from_txt(path)\n",
    "    m[\"Condition\"] = cond\n",
    "    rows_global.append(m)\n",
    "\n",
    "df_global = pd.DataFrame(rows_global)\n",
    "\n",
    "order = [\"D\", \"D'\", \"Low Resolution\", \"Motion Blur\", \"Low Light\"]\n",
    "df_global[\"order_idx\"] = df_global[\"Condition\"].apply(\n",
    "    lambda c: order.index(c) if c in order else len(order) + 1\n",
    ")\n",
    "df_global = (\n",
    "    df_global.sort_values(\"order_idx\")\n",
    "             .drop(columns=[\"order_idx\"])\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Global comparison (baseline only) ===\")\n",
    "display(df_global)\n",
    "\n",
    "\n",
    "\n",
    "# Figure 1: AP / AP50 / AP75\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "x_pos = range(len(df_global))\n",
    "x_labels = df_global[\"Condition\"]\n",
    "\n",
    "plt.plot(x_pos, df_global[\"AP\"],   marker=\"o\", label=\"AP@[.5:.95]\")\n",
    "plt.plot(x_pos, df_global[\"AP50\"], marker=\"o\", label=\"AP@0.5\")\n",
    "plt.plot(x_pos, df_global[\"AP75\"], marker=\"o\", label=\"AP@0.75\")\n",
    "\n",
    "plt.xticks(ticks=x_pos, labels=x_labels, rotation=20)\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Average Precision\")\n",
    "plt.title(\"AP metrics across different conditions\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: P / R / F1 \n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "bar_width = 0.25\n",
    "x_pos = range(len(df_global))\n",
    "\n",
    "plt.bar([x - bar_width for x in x_pos],\n",
    "        df_global[\"P\"],\n",
    "        width=bar_width,\n",
    "        label=\"P\")\n",
    "plt.bar(x_pos,\n",
    "        df_global[\"R\"],\n",
    "        width=bar_width,\n",
    "        label=\"R\")\n",
    "plt.bar([x + bar_width for x in x_pos],\n",
    "        df_global[\"F1\"],\n",
    "        width=bar_width,\n",
    "        label=\"F1\")\n",
    "\n",
    "plt.xticks(ticks=x_pos, labels=df_global[\"Condition\"], rotation=20)\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision / Recall / F1 across different conditions\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc306977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "METRICS_DIR = Path(\n",
    "    r\"E:\\2025 fall\\Fundamentals of Digital Image Processing\\YOLO11\\runs\\test_metrics\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_metrics_from_txt(path: Path) -> dict:\n",
    "    \"\"\"Parse one YOLO metrics txt file into a dict.\"\"\"\n",
    "    metrics = {\n",
    "        \"AP\":    None,\n",
    "        \"AP50\":  None,\n",
    "        \"AP75\":  None,\n",
    "        \"P\":     None,\n",
    "        \"R\":     None,\n",
    "        \"F1\":    None,\n",
    "    }\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            no_space = line.replace(\" \", \"\")\n",
    "\n",
    "            if \"AP@[.5:.95]\" in no_space or \"mAP@[.5:.95]\" in no_space:\n",
    "                metrics[\"AP\"] = float(line.split()[-1])\n",
    "            elif \"AP@0.5\" in no_space:\n",
    "                metrics[\"AP50\"] = float(line.split()[-1])\n",
    "            elif \"AP@0.75\" in no_space:\n",
    "                metrics[\"AP75\"] = float(line.split()[-1])\n",
    "            elif \"mean P(B)\" in line:\n",
    "                metrics[\"P\"] = float(line.split()[-1])\n",
    "            elif \"mean R(B)\" in line:\n",
    "                metrics[\"R\"] = float(line.split()[-1])\n",
    "            elif \"mean F1(B)\" in line:\n",
    "                metrics[\"F1\"] = float(line.split()[-1])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def add_bar_labels(ax, xs, ys, fmt=\"%.2f\", dy=0.015, fontsize=7):\n",
    "    \"\"\"Add small numeric labels above bars.\"\"\"\n",
    "    for x, y in zip(xs, ys):\n",
    "        if y is None or np.isnan(y):\n",
    "            continue\n",
    "        ax.text(\n",
    "            x,\n",
    "            y + dy,\n",
    "            fmt % y,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=fontsize,\n",
    "        )\n",
    "\n",
    "\n",
    "# mapping: each test set has distorted D' + traditional IP + ML model\n",
    "group_files = {\n",
    "    \"Low Resolution\": {\n",
    "        \"D'\":          \"metrics_test_low resolution.txt\",\n",
    "        \"Traditional\": \"metrics_test_low resolution_traditional.txt\",\n",
    "        \"FBCNN\":       \"metrics_test_low resolution_FBCNN.txt\",\n",
    "    },\n",
    "    \"Motion Blur\": {\n",
    "        \"D'\":          \"metrics_test_motion blur.txt\",\n",
    "        \"Traditional\": \"metrics_test_motion blur_traditional.txt\",\n",
    "        \"NAFNet\":      \"metrics_test_motion blur_NAFNet.txt\",\n",
    "    },\n",
    "    \"Low Light\": {\n",
    "        \"D'\":          \"metrics_test_low light.txt\",\n",
    "        \"Traditional\": \"metrics_test_low light_traditional.txt\",\n",
    "        \"ZeroDCE\":     \"metrics_test_low light_ZeroDCE.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "ap_data  = {k: {} for k in group_files}  # (AP, AP50, AP75)\n",
    "prf_data = {k: {} for k in group_files}  # (P, R, F1)\n",
    "\n",
    "for test_name, variants in group_files.items():\n",
    "    for method_label, fname in variants.items():\n",
    "        path = METRICS_DIR / fname\n",
    "        if not path.exists():\n",
    "            print(f\"[Warning] {path} not found, skip.\")\n",
    "            continue\n",
    "\n",
    "        m = parse_metrics_from_txt(path)\n",
    "        ap_data[test_name][method_label]  = (m[\"AP\"], m[\"AP50\"], m[\"AP75\"])\n",
    "        prf_data[test_name][method_label] = (m[\"P\"], m[\"R\"], m[\"F1\"])\n",
    "        print(\n",
    "            f\"[OK] {test_name} - {method_label}: \"\n",
    "            f\"AP={m['AP']:.3f}, AP50={m['AP50']:.3f}, AP75={m['AP75']:.3f}, \"\n",
    "            f\"P={m['P']:.3f}, R={m['R']:.3f}, F1={m['F1']:.3f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Figure 1: AP / AP50 / AP75\n",
    "ap_metric_labels = [\"AP@[.5:.95]\", \"AP@0.5\", \"AP@0.75\"]\n",
    "ap_colors        = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # blue / orange / green\n",
    "width = 0.22\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for ax, (test_name, variants) in zip(axes, group_files.items()):\n",
    "    method_names = list(variants.keys())\n",
    "    x = np.arange(len(method_names))\n",
    "\n",
    "    for k, (label, color) in enumerate(zip(ap_metric_labels, ap_colors)):\n",
    "        vals = []\n",
    "        xs = []\n",
    "        for idx, method in enumerate(method_names):\n",
    "            AP, AP50, AP75 = ap_data[test_name].get(\n",
    "                method, (np.nan, np.nan, np.nan)\n",
    "            )\n",
    "            if k == 0:\n",
    "                vals.append(AP)\n",
    "            elif k == 1:\n",
    "                vals.append(AP50)\n",
    "            else:\n",
    "                vals.append(AP75)\n",
    "            xs.append(x[idx] + (k - 1) * width)\n",
    "\n",
    "        ax.bar(\n",
    "            xs,\n",
    "            vals,\n",
    "            width=width,\n",
    "            color=color,\n",
    "            label=label if test_name == \"Low Resolution\" else None,\n",
    "        )\n",
    "        add_bar_labels(ax, xs, vals, fmt=\"%.2f\", dy=0.015, fontsize=7)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(method_names, rotation=20)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_title(test_name)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.supylabel(\"Average Precision\")\n",
    "fig.suptitle(\"AP metrics: D' vs Traditional vs ML\")\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "plt.tight_layout(rect=[0, 0, 0.98, 0.92])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: P / R / F1 \n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "metric_labels = [\"P\", \"R\", \"F1\"]\n",
    "metric_colors = {\"P\": \"#1f77b4\", \"R\": \"#ff7f0e\", \"F1\": \"#2ca02c\"}\n",
    "width = 0.22\n",
    "\n",
    "for ax, (test_name, variants) in zip(axes, group_files.items()):\n",
    "    method_names = list(variants.keys())\n",
    "    x = np.arange(len(method_names))\n",
    "\n",
    "    for j, metric in enumerate(metric_labels):\n",
    "        vals = []\n",
    "        xs = []\n",
    "        for idx, method in enumerate(method_names):\n",
    "            P, R, F1 = prf_data[test_name].get(\n",
    "                method, (np.nan, np.nan, np.nan)\n",
    "            )\n",
    "            if metric == \"P\":\n",
    "                vals.append(P)\n",
    "            elif metric == \"R\":\n",
    "                vals.append(R)\n",
    "            else:\n",
    "                vals.append(F1)\n",
    "            xs.append(x[idx] + (j - 1) * width)\n",
    "\n",
    "        ax.bar(\n",
    "            xs,\n",
    "            vals,\n",
    "            width=width,\n",
    "            color=metric_colors[metric],\n",
    "            label=metric if test_name == \"Low Resolution\" else None,\n",
    "        )\n",
    "        add_bar_labels(ax, xs, vals, fmt=\"%.2f\", dy=0.015, fontsize=7)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(method_names, rotation=20)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_title(test_name)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.supylabel(\"Score\")\n",
    "fig.suptitle(\"Precision / Recall / F1: D' vs Traditional vs ML\")\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "plt.tight_layout(rect=[0, 0, 0.98, 0.92])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Paths to finetune folders\n",
    "\n",
    "BASE_DIR = Path(r\"E:\\2025 fall\\Fundamentals of Digital Image Processing\\YOLO11\")\n",
    "\n",
    "finetune_folders = {\n",
    "    \"Low Light\":      BASE_DIR / \"finetune_low light\",\n",
    "    \"Low Resolution\": BASE_DIR / \"finetune_low resolution\",\n",
    "    \"Motion Blur\":    BASE_DIR / \"finetune_motion blur\",\n",
    "}\n",
    "\n",
    "\n",
    "# Helper: load train loss from results.csv\n",
    "\n",
    "def load_train_loss(csv_path: Path):\n",
    "    \"\"\"\n",
    "    Load training loss curve from a YOLO results.csv.\n",
    "\n",
    "    Priority:\n",
    "    1) If column 'train/loss' exists, use it directly.\n",
    "    2) Otherwise, sum all columns that contain both 'train' and 'loss'.\n",
    "       This works for formats like 'train/box_loss', 'train/cls_loss', etc.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # x-axis: epoch if available, otherwise just use row index\n",
    "    if \"epoch\" in df.columns:\n",
    "        x = df[\"epoch\"].values\n",
    "    else:\n",
    "        x = range(len(df))\n",
    "\n",
    "    # choose loss column(s)\n",
    "    if \"train/loss\" in df.columns:\n",
    "        loss = df[\"train/loss\"].values\n",
    "    else:\n",
    "        loss_cols = [c for c in df.columns if (\"train\" in c and \"loss\" in c)]\n",
    "        if not loss_cols:\n",
    "            raise ValueError(f\"No train loss columns found in {csv_path}.\\n\"\n",
    "                             f\"Available columns: {list(df.columns)}\")\n",
    "        loss = df[loss_cols].sum(axis=1).values\n",
    "\n",
    "    return x, loss\n",
    "\n",
    "\n",
    "# Collect curves for each finetune run\n",
    "\n",
    "curves = {}\n",
    "\n",
    "for label, folder in finetune_folders.items():\n",
    "    csv_path = folder / \"results.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[Warning] {csv_path} not found, skip.\")\n",
    "        continue\n",
    "\n",
    "    x, loss = load_train_loss(csv_path)\n",
    "    curves[label] = (x, loss)\n",
    "    print(f\"[OK] Loaded {label} from {csv_path}\")\n",
    "\n",
    "\n",
    "# Plot all three curves in one figure\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "colors = {\n",
    "    \"Low Light\":      \"#1f77b4\",  # blue\n",
    "    \"Low Resolution\": \"#ff7f0e\",  # orange\n",
    "    \"Motion Blur\":    \"#2ca02c\",  # green\n",
    "}\n",
    "\n",
    "for label, (x, loss) in curves.items():\n",
    "    plt.plot(x, loss, label=label,\n",
    "             linewidth=2,\n",
    "             marker=\"o\",\n",
    "             markersize=4,\n",
    "             color=colors.get(label, None))\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.title(\"Training loss curves for fine-tuned models\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "# Paths & file loading\n",
    "\n",
    "METRICS_DIR = Path(\n",
    "    r\"E:\\2025 fall\\Fundamentals of Digital Image Processing\\YOLO11\\runs\\test_metrics\"\n",
    ")\n",
    "\n",
    "txt_files = sorted(METRICS_DIR.glob(\"*.txt\"))\n",
    "print(\"Found txt files:\")\n",
    "for f in txt_files:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def detect_testset(name_low: str):\n",
    "    \"\"\"Decide which test set this file belongs to based on its filename.\"\"\"\n",
    "    if \"low resolution\" in name_low:\n",
    "        return \"Low Resolution\"\n",
    "    if \"motion blur\" in name_low:\n",
    "        return \"Motion Blur\"\n",
    "    if \"low light\" in name_low:\n",
    "        return \"Low Light\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_variant(name_low: str):\n",
    "    \"\"\"\n",
    "    Decide which variant it is, based on its filename:\n",
    "      - Fine-Tuned      (pure finetuning, no extra IP method)\n",
    "      - Traditional     (contains 'traditional')\n",
    "      - ML              (NAFNet / FBCNN / ZeroDCE etc.)\n",
    "    \"\"\"\n",
    "    # Traditional IP methods\n",
    "    if \"traditional\" in name_low:\n",
    "        return \"Traditional\"\n",
    "\n",
    "    # Learning-based IP methods\n",
    "    if (\"nafnet\" in name_low) or (\"fbcnn\" in name_low) or (\"zerodce\" in name_low):\n",
    "        return \"ML\"\n",
    "\n",
    "    # Pure finetuning (no IP keyword)\n",
    "    if (\"finetune\" in name_low) or (\"fine tune\" in name_low):\n",
    "        return \"Fine-Tuned\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_metrics_from_txt(path: Path):\n",
    "    \"\"\"\n",
    "    Parse one metrics txt file, e.g.\n",
    "\n",
    "    AP@[.5:.95](B): 0.767013\n",
    "    AP@0.5(B)     : 0.902827\n",
    "    AP@0.75(B)    : 0.830418\n",
    "    mean P(B)     : 0.865376\n",
    "    mean R(B)     : 0.910000\n",
    "    mean F1(B)    : 0.887127\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"AP\":   None,\n",
    "        \"AP50\": None,\n",
    "        \"AP75\": None,\n",
    "        \"P\":    None,\n",
    "        \"R\":    None,\n",
    "        \"F1\":   None,\n",
    "    }\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            no_space = line.replace(\" \", \"\")\n",
    "\n",
    "            if (\"AP@[.5:.95]\" in no_space) or (\"mAP@[.5:.95]\" in no_space):\n",
    "                metrics[\"AP\"] = float(line.split()[-1])\n",
    "            elif \"AP@0.5\" in no_space:\n",
    "                metrics[\"AP50\"] = float(line.split()[-1])\n",
    "            elif \"AP@0.75\" in no_space:\n",
    "                metrics[\"AP75\"] = float(line.split()[-1])\n",
    "            elif \"meanP(B)\" in no_space:\n",
    "                metrics[\"P\"] = float(line.split()[-1])\n",
    "            elif \"meanR(B)\" in no_space:\n",
    "                metrics[\"R\"] = float(line.split()[-1])\n",
    "            elif \"meanF1(B)\" in no_space:\n",
    "                metrics[\"F1\"] = float(line.split()[-1])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Small helper: add value labels on top of bars\n",
    "def add_bar_labels(ax, xs, ys, fmt=\"%.2f\", dy=0.01, fontsize=7):\n",
    "    \"\"\"\n",
    "    Add numeric value labels above bars.\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib axis.\n",
    "        xs: x positions of bars.\n",
    "        ys: heights of bars.\n",
    "        fmt: format string for values.\n",
    "        dy: vertical offset to avoid overlap with bar edge.\n",
    "        fontsize: text font size.\n",
    "    \"\"\"\n",
    "    for x, y in zip(xs, ys):\n",
    "        if y is None or np.isnan(y):\n",
    "            continue\n",
    "        ax.text(\n",
    "            x,\n",
    "            y + dy,\n",
    "            fmt % y,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=fontsize,\n",
    "        )\n",
    "\n",
    "\n",
    "# Build DataFrame\n",
    "\n",
    "rows = []\n",
    "\n",
    "for path in txt_files:\n",
    "    name_low = path.stem.lower()\n",
    "\n",
    "    # Keep only finetune-related results\n",
    "    if (\"finetune\" not in name_low) and (\"fine tune\" not in name_low):\n",
    "        continue\n",
    "\n",
    "    testset = detect_testset(name_low)\n",
    "    variant = detect_variant(name_low)\n",
    "\n",
    "    if (testset is None) or (variant is None):\n",
    "        continue\n",
    "\n",
    "    metrics = parse_metrics_from_txt(path)\n",
    "    metrics[\"TestSet\"] = testset\n",
    "    metrics[\"Variant\"] = variant\n",
    "    rows.append(metrics)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "test_order = [\"Low Resolution\", \"Motion Blur\", \"Low Light\"]\n",
    "variant_order = [\"Fine-Tuned\", \"Traditional\", \"ML\"]\n",
    "\n",
    "df[\"test_idx\"] = df[\"TestSet\"].apply(\n",
    "    lambda s: test_order.index(s) if s in test_order else 999\n",
    ")\n",
    "df[\"variant_idx\"] = df[\"Variant\"].apply(\n",
    "    lambda s: variant_order.index(s) if s in variant_order else 999\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.sort_values([\"test_idx\", \"variant_idx\"])\n",
    "      .reset_index(drop=True)\n",
    "      .drop(columns=[\"test_idx\", \"variant_idx\"])\n",
    ")\n",
    "\n",
    "print(\"\\nParsed metrics for finetune comparison:\")\n",
    "display(df)\n",
    "\n",
    "# Display name for the ML method on x-axis for each test set\n",
    "ml_label_map = {\n",
    "    \"Low Resolution\": \"FBCNN\",\n",
    "    \"Motion Blur\": \"NAFNet\",\n",
    "    \"Low Light\": \"ZeroDCE\",\n",
    "}\n",
    "\n",
    "\n",
    "# Figure 1: AP / AP@0.5 / AP@0.75 bar plots\n",
    "\n",
    "\n",
    "\n",
    "ap_metrics_order = [\"AP\", \"AP50\", \"AP75\"]\n",
    "ap_labels = {\n",
    "    \"AP\":   \"AP@[.5:.95]\",\n",
    "    \"AP50\": \"AP@0.5\",\n",
    "    \"AP75\": \"AP@0.75\",\n",
    "}\n",
    "ap_colors = {\n",
    "    \"AP\":   \"tab:blue\",\n",
    "    \"AP50\": \"tab:orange\",\n",
    "    \"AP75\": \"tab:green\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(test_order), figsize=(12, 4), sharey=True)\n",
    "bar_width = 0.18  # thinner bars\n",
    "\n",
    "for i, tset in enumerate(test_order):\n",
    "    ax = axes[i]\n",
    "    sub = df[df[\"TestSet\"] == tset].set_index(\"Variant\")\n",
    "\n",
    "    base_positions = list(range(len(variant_order)))  # 0,1,2\n",
    "\n",
    "    # Draw three groups of AP-related bars\n",
    "    for m_idx, metric in enumerate(ap_metrics_order):\n",
    "        ys = []\n",
    "        xs = []\n",
    "        offset = (m_idx - 1) * bar_width  # -1,0,1\n",
    "        for v_idx, v in enumerate(variant_order):\n",
    "            if v in sub.index and pd.notna(sub.loc[v, metric]):\n",
    "                val = sub.loc[v, metric]\n",
    "                if isinstance(val, pd.Series):\n",
    "                    val = val.iloc[0]\n",
    "                ys.append(float(val))\n",
    "            else:\n",
    "                ys.append(float(\"nan\"))\n",
    "            xs.append(base_positions[v_idx] + offset)\n",
    "\n",
    "        ax.bar(\n",
    "            xs,\n",
    "            ys,\n",
    "            width=bar_width,\n",
    "            color=ap_colors[metric],\n",
    "        )\n",
    "        # Add value labels for this metric\n",
    "        add_bar_labels(ax, xs, ys, fmt=\"%.2f\", dy=0.015, fontsize=7)\n",
    "\n",
    "    # X-tick labels: Fine-Tuned / Traditional / specific ML method\n",
    "    xtick_labels = []\n",
    "    for v in variant_order:\n",
    "        if v == \"ML\":\n",
    "            xtick_labels.append(ml_label_map.get(tset, \"ML\"))\n",
    "        else:\n",
    "            xtick_labels.append(v)\n",
    "\n",
    "    ax.set_xticks(base_positions)\n",
    "    ax.set_xticklabels(xtick_labels, rotation=20)\n",
    "    ax.set_title(tset)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Score\")\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Global legend on the top-right\n",
    "ap_handles = [Patch(color=ap_colors[m], label=ap_labels[m]) for m in ap_metrics_order]\n",
    "fig.legend(\n",
    "    handles=ap_handles,\n",
    "    title=\"AP Metric\",\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(1, 0.98),\n",
    "    ncol=1,\n",
    "    borderaxespad=0.3,\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"AP metrics comparison: Fine-Tuning vs IP\",\n",
    "    y=0.96,\n",
    ")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: P / R / F1 bar plots\n",
    "\n",
    "metrics_order = [\"P\", \"R\", \"F1\"]\n",
    "metrics_colors = {\n",
    "    \"P\": \"tab:blue\",\n",
    "    \"R\": \"tab:orange\",\n",
    "    \"F1\": \"tab:green\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(test_order), figsize=(12, 4), sharey=True)\n",
    "bar_width = 0.18  # thinner bars\n",
    "\n",
    "for i, tset in enumerate(test_order):\n",
    "    ax = axes[i]\n",
    "    sub = df[df[\"TestSet\"] == tset].set_index(\"Variant\")\n",
    "\n",
    "    base_positions = list(range(len(variant_order)))\n",
    "\n",
    "    for m_idx, metric in enumerate(metrics_order):\n",
    "        ys = []\n",
    "        xs = []\n",
    "        offset = (m_idx - 1) * bar_width  # -1,0,1\n",
    "        for v_idx, v in enumerate(variant_order):\n",
    "            if v in sub.index and pd.notna(sub.loc[v, metric]):\n",
    "                val = sub.loc[v, metric]\n",
    "                if isinstance(val, pd.Series):\n",
    "                    val = val.iloc[0]\n",
    "                ys.append(float(val))\n",
    "            else:\n",
    "                ys.append(float(\"nan\"))\n",
    "            xs.append(base_positions[v_idx] + offset)\n",
    "\n",
    "        ax.bar(\n",
    "            xs,\n",
    "            ys,\n",
    "            width=bar_width,\n",
    "            color=metrics_colors[metric],\n",
    "        )\n",
    "        # Add value labels for this metric\n",
    "        add_bar_labels(ax, xs, ys, fmt=\"%.2f\", dy=0.015, fontsize=7)\n",
    "\n",
    "    # X-axis labels: replace \"ML\" with actual method name\n",
    "    xtick_labels = []\n",
    "    for v in variant_order:\n",
    "        if v == \"ML\":\n",
    "            xtick_labels.append(ml_label_map.get(tset, \"ML\"))\n",
    "        else:\n",
    "            xtick_labels.append(v)\n",
    "\n",
    "    ax.set_xticks(base_positions)\n",
    "    ax.set_xticklabels(xtick_labels, rotation=20)\n",
    "    ax.set_title(tset)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Score\")\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Global legend for P/R/F1\n",
    "handles = [Patch(color=metrics_colors[m], label=m) for m in metrics_order]\n",
    "fig.legend(\n",
    "    handles=handles,\n",
    "    title=\"Metric\",\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(0.98, 0.98),\n",
    "    ncol=1,\n",
    "    borderaxespad=0.3,\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"P / R / F1 comparison: Fine-Tuning vs IP\",\n",
    "    y=0.96,\n",
    ")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
